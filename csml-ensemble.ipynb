{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T21:27:06.745516Z",
     "start_time": "2025-03-19T21:27:06.744127Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:27:09.959930Z",
     "start_time": "2025-03-19T21:27:06.787356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, fbeta_score, classification_report\n",
    "from imblearn.metrics import geometric_mean_score \n",
    "import torch\n",
    "\n",
    "from peft import PeftModel, prepare_model_for_kbit_training, PeftConfig, AutoPeftModelForSequenceClassification, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import skopt\n",
    "\n",
    "# uploading to HuggingFace \n",
    "from huggingface_hub import login\n",
    "\n",
    "login(new_session=False, # Won’t request token if one is already saved on machine\n",
    "write_permission=True, # Requires a token with write permission\n",
    "token=os.environ["HF_TOKEN"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n"
   ],
   "id": "c6525ac8bbabfe7b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/csml-ensemble/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/csml-ensemble/venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in 'login': write_permission. Will not be supported from version '1.0'.\n",
      "\n",
      "Fine-grained tokens added complexity to the permissions, making it irrelevant to check if a token has 'write' access.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:27:12.259323Z",
     "start_time": "2025-03-19T21:27:10.018848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "cwes = ['672', '119', '682', '754']\n",
    "models = ['codebert', 'codeT5']\n",
    "pipelines = []\n",
    "\n",
    "def load_model(cwe: int, model_name: str):\n",
    "    repo_id = f\"Vulnerability-Detection/cwe{cwe}-{model_name}\"\n",
    "    os.makedirs(f\"./kaggle/working/{cwe}_{model_name}/\", exist_ok=True)\n",
    "    folder = snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=f\"./kaggle/working/{cwe}_{model_name}/\",\n",
    "    )\n",
    "\n",
    "    return folder\n",
    "\n",
    "\n",
    "for cwe in cwes:\n",
    "    for model in models:\n",
    "        folder = load_model(cwe=cwe, model_name=model)\n",
    "        if torch.cuda.is_available():\n",
    "            tokenizer = AutoTokenizer.from_pretrained(folder, local_files_only=True,) \n",
    "            merged_model = AutoModelForSequenceClassification.from_pretrained(folder).to(device) \n",
    "            merged_model.to(device)\n",
    "        else:\n",
    "            merged_model = None\n",
    "            tokenizer = None\n",
    "        pipelines.append([tokenizer, merged_model, model, cwe])"
   ],
   "id": "ade12117985da782",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 146312.93it/s]\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 39850.87it/s]\n",
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 14938.16it/s]\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 14627.04it/s]\n",
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 10285.76it/s]\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 31926.20it/s]\n",
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 29935.56it/s]\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 30504.03it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b18c93e607cc419d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:27:14.022069Z",
     "start_time": "2025-03-19T21:27:12.270705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "# import tqdm\n",
    "tqdm.pandas()  \n",
    "\n",
    "def collect_predictions(X, pipeline):\n",
    "    # The pipe line contains a tokenizer and a model\n",
    "    tokenizer = pipeline[0]\n",
    "    model = pipeline[1]\n",
    "    tokenize = lambda func: tokenizer(func ,padding=\"max_length\",truncation=True, return_tensors=\"pt\").to(device)\n",
    "    tokenized_text = X.progress_map(tokenize)\n",
    "\n",
    "    def process_with_model(tokenized_input):\n",
    "        outputs = model(**tokenized_input)\n",
    "        logits = outputs.logits\n",
    "        logits = logits.detach().cpu().numpy().flatten()\n",
    "        return logits\n",
    "\n",
    "    predictions = tokenized_text.progress_map(process_with_model)\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "labels = []\n",
    "test_df = pd.read_csv(\"./kaggle/input/vulnerabilities-dataset/test_all.csv\")\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    model, cwe = pipeline[2], pipeline[3]\n",
    "    labels.extend([f\"{model}_{cwe}_0\", f\"{model}_{cwe}_1\"])\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\"Vulnerability-Detection/test_set_llm_predictions\")\n",
    "    df = dataset['train'].to_pandas()\n",
    "    X = df.drop([\"vulnerable\", \"__index_level_0__\"], axis=1, errors='ignore',)\n",
    "    y = df['vulnerable']\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Failed to pull\")\n",
    "    print(e)\n",
    "    predictions = []\n",
    "    for pipeline in pipelines:\n",
    "        predictions.append(collect_predictions(test_df['func'], pipeline))\n",
    "\n",
    "    predictions = np.hstack([np.vstack(row) for row in np.array(predictions)])\n",
    "    X = pd.DataFrame(predictions, columns=labels)\n",
    "    y = pd.Series(test_df['vulnerable'])\n",
    "\n",
    "X.to_csv(\"X_all.csv\")\n",
    "y.to_csv(\"y_all.csv\")\n",
    "\n",
    "dataset = X.copy()\n",
    "dataset['vulnerable'] = y.copy()\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "dataset.push_to_hub(\"Vulnerability-Detection/test_set_llm_predictions\")"
   ],
   "id": "4a3ac69350d9d0ca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 33/33 [00:00<00:00, 1209.67ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Vulnerability-Detection/test_set_llm_predictions/commit/10f199643f84a057760171181bcd60bb26e805ff', commit_message='Upload dataset', commit_description='', oid='10f199643f84a057760171181bcd60bb26e805ff', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Vulnerability-Detection/test_set_llm_predictions', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Vulnerability-Detection/test_set_llm_predictions'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "25c52e1915cc5853"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:27:14.076159Z",
     "start_time": "2025-03-19T21:27:14.040618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from skopt.space import  Integer, Categorical, Real\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, RUSBoostClassifier\n",
    "\n",
    "MODELS = {\n",
    "    \"DecisionTree\": {\n",
    "        \"estimator\": DecisionTreeClassifier(),\n",
    "        \"search_spaces\": {\n",
    "            \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "            \"max_depth\": Integer(3, 50),\n",
    "            \"max_features\": Categorical([\"sqrt\", \"log2\"]),\n",
    "            \"min_samples_leaf\": Integer(1, 10),\n",
    "            \"min_samples_split\": Integer(2, 10),\n",
    "        },\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"estimator\": RandomForestClassifier(),\n",
    "        \"search_spaces\": {\n",
    "            \"bootstrap\": Categorical([True, False]),\n",
    "            \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "            \"max_depth\": Integer(3, 50),\n",
    "            \"max_features\": Categorical([\"sqrt\", \"log2\"]),\n",
    "            \"min_samples_leaf\": Integer(1, 10),\n",
    "            \"min_samples_split\": Integer(2, 10),\n",
    "            \"n_estimators\": Integer(50, 500),\n",
    "        },\n",
    "    },\n",
    "    \"BalancedBagging\": {\n",
    "        \"estimator\": BalancedBaggingClassifier(),\n",
    "        \"search_spaces\": {\n",
    "            \"warm_start\": Categorical([True, False]),\n",
    "            \"bootstrap\": Categorical([True, False]),\n",
    "            \"n_estimators\": Integer(5, 50),\n",
    "            \"sampling_strategy\": Real(0.1, 1, prior=\"log-uniform\"), # Controls sampling rate for RUS.\n",
    "        },\n",
    "    },\n",
    "    \"BalancedRandomForest\": {\n",
    "        \"estimator\": BalancedRandomForestClassifier(),\n",
    "        \"search_spaces\": {\n",
    "            \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "            \"max_depth\": Integer(3, 50),\n",
    "            \"max_features\": Categorical([\"sqrt\", \"log2\"]),\n",
    "            \"min_samples_leaf\": Integer(1, 10),\n",
    "            \"min_samples_split\": Integer(2, 10),\n",
    "            \"n_estimators\": Integer(50, 500),\n",
    "            \"sampling_strategy\": Real(0.1, 1, prior=\"log-uniform\"), # Controls sampling rate for RUS.\n",
    "        },\n",
    "    },\n",
    "   \"XGBoost\": {\n",
    "        \"estimator\": XGBClassifier(),\n",
    "        \"search_spaces\": {\n",
    "            \"max_depth\": Integer(1, 10),\n",
    "            \"gamma\": Real(0.1, 10, prior=\"log-uniform\"),\n",
    "            \"subsample\": Real(0.5, 1, prior=\"log-uniform\"),\n",
    "            \"min_child_weight\": Integer(1, 10),\n",
    "            \"colsample_bytree\": Real(0.5, 1, prior=\"log-uniform\"),\n",
    "            \"learning_rate\": Real(0.1, 1, prior=\"log-uniform\"),\n",
    "            \"max_delta_step\": Integer(0, 10),\n",
    "            \"lambda\": Integer(1, 3),\n",
    "            \"alpha\": Integer(0, 2),\n",
    "        },\n",
    "    },\n",
    "    \"RUSBoost\": {\n",
    "        \"estimator\": RUSBoostClassifier(),\n",
    "        \"search_spaces\": {\n",
    "            \"learning_rate\": Real(0.1, 1, prior=\"log-uniform\"),\n",
    "            \"n_estimators\": Integer(10, 500),\n",
    "            \"sampling_strategy\": Real(0.1, 1, prior=\"log-uniform\"), # Controls sampling rate for RUS.\n",
    "        },\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"estimator\": AdaBoostClassifier(),\n",
    "        \"search_spaces\": {\n",
    "            \"learning_rate\": Real(0.1, 1, prior=\"log-uniform\"),\n",
    "            \"n_estimators\": Integer(10, 500),\n",
    "        },\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"estimator\": make_pipeline(StandardScaler(), SVC(max_iter=100000)),\n",
    "        \"search_spaces\": {\n",
    "            \"svc__C\": Real(0.1, 100, prior=\"log-uniform\"),\n",
    "            \"svc__gamma\": Real(0.1, 10, prior=\"log-uniform\"),\n",
    "            \"svc__degree\": Integer(1, 5),\n",
    "            \"svc__kernel\": Categorical([\"rbf\", \"poly\", \"sigmoid\"]),\n",
    "            \"svc__class_weight\": Categorical([\"balanced\", None]),\n",
    "        },\n",
    "    },\n",
    "}"
   ],
   "id": "8186a35304eb166f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e3239c08408eb376"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-19T21:27:14.098378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "X\n",
    "\n",
    "cvf = StratifiedKFold(n_splits=5)\n",
    "results = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cvf.split(X, y)):\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "    \n",
    "    for label, setup in MODELS.items():\n",
    "        if label != \"RUSBoost\":\n",
    "            continue\n",
    "        search_space = setup['search_spaces']\n",
    "        estimator = setup['estimator']\n",
    "    \n",
    "        model = BayesSearchCV(\n",
    "            estimator=estimator,\n",
    "            search_spaces=search_space,\n",
    "            cv=5,\n",
    "            scoring=\"f1\",\n",
    "            refit=True,\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        beta = 2\n",
    "        fbeta = fbeta_score(y_test, y_pred, beta=beta)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        print(f\"Fold {i}, {label} scored F{beta} {fbeta}, Precision {precision}, Recall {recall}\")\n",
    "        results.append([i, label, fbeta, precision, recall])\n",
    "\n",
    "np.array(results)[:, 2:].mean()"
   ],
   "id": "c0d72c09c88ac125",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
